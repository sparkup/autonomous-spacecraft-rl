{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debc3b60",
   "metadata": {},
   "source": [
    "# Launch 3 - Deep Q-Network (DQN)\n",
    "\n",
    "This phase generalizes Q-Learning by replacing the table with a neural network.\n",
    "\n",
    "## Objectives\n",
    "- Introduce functional approximation with Q_theta(s,a)\n",
    "- Understand the role of replay buffer and target network\n",
    "- Train a DQN agent on CartPole-v1 with Stable-Baselines3\n",
    "\n",
    "## Chronological positioning\n",
    "Launch 1: random baseline -> Launch 2: tabular RL -> Launch 3: deep RL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2406a",
   "metadata": {},
   "source": [
    "## Scientific Justification (Phase 3)\n",
    "\n",
    "### Why DQN in Launch 3, and why move to PPO afterward?\n",
    "DQN is used here to introduce **deep function approximation** with replay and target networks. It is a key transitional step from tabular methods to deep RL.\n",
    "\n",
    "For the mission phase, we move to PPO because:\n",
    "- PPO is typically more stable in policy optimization\n",
    "- On-policy updates reduce stale-target effects from replay\n",
    "- Clipped objective limits destructive policy jumps\n",
    "- It is widely adopted for robust control-like benchmark tasks\n",
    "\n",
    "### Why LunarLander for the mission trajectory?\n",
    "LunarLander is selected as a structured RL benchmark with:\n",
    "- Nonlinear dynamics\n",
    "- Reward shaping already aligned with landing behavior\n",
    "- Interpretable state/action interfaces\n",
    "- Good reproducibility for controlled comparisons\n",
    "\n",
    "It is not a real spacecraft simulator, but a useful **guidance-and-landing proxy**.\n",
    "\n",
    "### Reward, variance, and measurement discipline\n",
    "In this phase we explicitly track:\n",
    "- Mean reward (central performance)\n",
    "- Reward standard deviation (stability)\n",
    "- Repeated evaluation episodes for robustness\n",
    "\n",
    "This prepares mission-level reporting with smoothed trends and confidence-aware interpretation.\n",
    "\n",
    "### Known limitations and Sim2Real gap\n",
    "Current setup still omits:\n",
    "- Sensor noise models\n",
    "- Actuator saturation nonlinearities\n",
    "- Delays, faults, and uncertainty envelopes\n",
    "- Realistic environment disturbances\n",
    "\n",
    "A credible Sim2Real path would include domain randomization, robustness stress testing, safety constraints, and transfer validation before any operational claim.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed53e0",
   "metadata": {},
   "source": [
    "### A - Value network and replay memory\n",
    "\n",
    "Two components stabilize DQN training:\n",
    "1. **Q-network** to estimate action values\n",
    "2. **Replay buffer** to reduce temporal correlation between transitions\n",
    "\n",
    "Mini-batch learning improves numerical stability compared to purely sequential updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875a20d",
   "metadata": {},
   "source": [
    "### B - DQN training loop (conceptual view)\n",
    "\n",
    "The loop alternates:\n",
    "- Action selection\n",
    "- Transition collection\n",
    "- Mini-batch updates\n",
    "- Periodic target network synchronization\n",
    "\n",
    "This architecture mitigates classical instability in deep Q-Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844de885",
   "metadata": {},
   "source": [
    "### C - Training with Stable-Baselines3\n",
    "\n",
    "We rely on SB3 for robust experimentation:\n",
    "- Algorithm configuration\n",
    "- Optimization process\n",
    "- Model persistence and reload\n",
    "\n",
    "The scientific workflow remains the same: define, train, evaluate, interpret.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9996369",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Use a non-rendering env for training stability\n",
    "train_env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Create the DQN model\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=32,\n",
    "    learning_starts=1000,\n",
    "    target_update_interval=500,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=50_000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"dqn_cartpole\")\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# Close training resources once learning is done\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2a9ba",
   "metadata": {},
   "source": [
    "### D - Quantitative evaluation\n",
    "\n",
    "We report mean reward and standard deviation over independent episodes.\n",
    "\n",
    "Interpretation:\n",
    "- High mean = strong central performance\n",
    "- Low standard deviation = stable behavior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1e4e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = Monitor(gym.make(\"CartPole-v1\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, render=False)\n",
    "print(f\"Average reward : {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d17a",
   "metadata": {},
   "source": [
    "### E - Qualitative behavior inspection\n",
    "\n",
    "Visual trajectory analysis complements numerical metrics:\n",
    "- Verify dynamic consistency\n",
    "- Detect edge-case behavior\n",
    "- Prepare transfer to more demanding control environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787a8cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "render_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "obs, _ = render_env.reset()\n",
    "frames = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = render_env.step(action)\n",
    "    frame = render_env.render()   # returns a NumPy image\n",
    "    frames.append(frame)\n",
    "    done = terminated or truncated\n",
    "\n",
    "render_env.close()\n",
    "\n",
    "# Show the first frame\n",
    "plt.imshow(frames[0])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9a8e1",
   "metadata": {},
   "source": [
    "## Launch 3 Conclusion\n",
    "\n",
    "We validated the transition to deep RL.\n",
    "\n",
    "### Key outcomes\n",
    "- Successful replacement of Q-Table\n",
    "- Reproducible training and evaluation pipeline\n",
    "- Improved suitability for continuous/high-dimensional states\n",
    "\n",
    "### Mission transition\n",
    "The Mission phase applies these principles to LunarLander-v3 with a PPO policy, better suited to this spacecraft control context.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
