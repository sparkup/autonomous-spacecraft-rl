{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03162571",
   "metadata": {},
   "source": [
    "# Launch 2 - Tabular Q-Learning\n",
    "\n",
    "After the random baseline, this phase introduces a learning agent based on a **Q-Table**.\n",
    "\n",
    "## Objectives\n",
    "- Formulate the problem in a discrete state-action setting\n",
    "- Implement the Bellman update in Q-Learning\n",
    "- Quantify performance gain versus a non-trained policy\n",
    "\n",
    "## Experimental setting\n",
    "We use **FrozenLake-v1**, which is discrete and suitable for direct analysis of Q(s,a) values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7d7a3",
   "metadata": {},
   "source": [
    "## Scientific Justification (Phase 2)\n",
    "\n",
    "### Why Tabular Q-Learning here?\n",
    "Launch 2 introduces a transparent, interpretable learning baseline:\n",
    "- Explicit value table updates\n",
    "- Clear exploration/exploitation dynamics\n",
    "- Direct convergence diagnostics\n",
    "\n",
    "This creates a traceable bridge between random behavior (Launch 1) and function approximation (Launch 3).\n",
    "\n",
    "### PPO status in this phase\n",
    "PPO is intentionally postponed here. The goal is to verify that policy improvement and Bellman updates are correctly understood and measured before switching to neural policies.\n",
    "\n",
    "### Reward shaping and objective clarity\n",
    "The reward signal is analyzed as a design object, not only as a score. We examine whether reward structure actually incentivizes task completion and stable behavior.\n",
    "\n",
    "### Variance and robustness controls\n",
    "Variance is managed through:\n",
    "- Multiple evaluation episodes\n",
    "- Success-rate reporting\n",
    "- Stable protocol settings\n",
    "\n",
    "These controls are prerequisites for meaningful comparison with deep RL phases.\n",
    "\n",
    "### Simulation limitations and transfer caution\n",
    "As a tabular benchmark phase, it does not model mission-level nonlinear guidance or real-world disturbances. Conclusions remain algorithmic and methodological, not operational-flight claims.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02807725",
   "metadata": {},
   "source": [
    "### A - Environment and Q-Table preparation\n",
    "\n",
    "We initialize:\n",
    "- Number of states S\n",
    "- Number of actions A\n",
    "- A Q table of shape (|S| x |A|), initialized to zeros\n",
    "\n",
    "This table is a tabular approximation of expected action value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ccef0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Create FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Q-Table dimensions\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Initialize Q-Table\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "print(\"Number of states:\", n_states)\n",
    "print(\"Number of actions:\", n_actions)\n",
    "print(\"Initial Q-Table:\", Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9fbc4c",
   "metadata": {},
   "source": [
    "### B - Q-Learning loop (exploration vs exploitation)\n",
    "\n",
    "At each transition, we apply an epsilon-greedy policy and the update:\n",
    "\n",
    "Q(s,a) <- Q(s,a) + alpha * [r + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "\n",
    "Interpretation:\n",
    "- `Alpha` controls adaptation speed\n",
    "- `Gamma` weights long-term return\n",
    "- `Epsilon` enforces sufficient exploration\n",
    "\n",
    "This mechanism progressively converges to a stronger policy in discrete environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d04f2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.8       # learning rate\n",
    "gamma = 0.95      # discount factor\n",
    "epsilon = 0.2     # exploration rate\n",
    "n_episodes = 5000\n",
    "max_steps = 100\n",
    "\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "        \n",
    "        # Environment step\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Q-Table update\n",
    "        Q[state, action] = Q[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q[new_state, :]) - Q[state, action]\n",
    "        )\n",
    "        \n",
    "        state = new_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print(\"Q-Table after training:\", Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ade170",
   "metadata": {},
   "source": [
    "### C - Trained policy evaluation\n",
    "\n",
    "The agent is evaluated in greedy mode (argmax over Q) across many episodes.\n",
    "\n",
    "Primary metric: **success rate**.\n",
    "\n",
    "This metric is robust in FrozenLake because terminal reward is binary (success/failure).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb756a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate trained agent\n",
    "n_eval_episodes = 1000\n",
    "successes = 0\n",
    "\n",
    "for episode in range(n_eval_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Greedy action from Q-Table\n",
    "        action = np.argmax(Q[state, :])\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Count success when terminal reward is 1\n",
    "        if reward == 1.0:\n",
    "            successes += 1\n",
    "\n",
    "# Compute success rate\n",
    "success_rate = successes / n_eval_episodes\n",
    "print(f\"Success rate over {n_eval_episodes} episodes: {success_rate*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b174fcf",
   "metadata": {},
   "source": [
    "## Launch 2 Conclusion\n",
    "\n",
    "This phase demonstrates the first major step forward:\n",
    "- From random policy to learned policy\n",
    "- Measurable improvement through success rate\n",
    "- Validation of dynamic programming principles in RL\n",
    "\n",
    "### Identified limitation\n",
    "Tabular Q-Learning does not scale well to continuous or high-dimensional state spaces.\n",
    "\n",
    "### Phase transition\n",
    "Launch 3 replaces the table with a neural approximation model (**DQN**), required for more realistic systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
